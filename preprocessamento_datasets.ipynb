{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBsPbpk5Hhte"
      },
      "outputs": [],
      "source": [
        "#download da biblioteca pycatch22 para extracao dos 22 atributos do dataset\n",
        "!pip install pycatch22\n",
        "import pycatch22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pme8CTQeHhti"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N03S4GQ1JOUD"
      },
      "outputs": [],
      "source": [
        "# importacoes\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import threading\n",
        "from scipy import signal\n",
        "\n",
        "root = \"C:\\\\Users\\\\radek\\\\Desktop\\\\PUC\\\\tcc\\\\\"\n",
        "directory = root+\"Rapid_age-grading_and_species_identification_of_natural_mosquitoes\\\\\"\n",
        "# directory = root+\"1. IR Spectra of Mosquitoes/\"\n",
        "files_directory = os.listdir(directory)\n",
        "dataset = pd.DataFrame()\n",
        "dataset_resampled = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL1MyAqCQcnL"
      },
      "outputs": [],
      "source": [
        "# funcao para normalizacao dos valores de absorcao\n",
        "def znorm(x):\n",
        "  x_znorm = (x - np.mean(x)) / np.std(x)\n",
        "  return x_znorm\n",
        "\n",
        "#funcao para atribuir o valor mais proximo aos faltantes\n",
        "def closest_index(grouped, index):\n",
        "    left = grouped.index[grouped.index < index].max()\n",
        "    right = grouped.index[grouped.index > index].min()\n",
        "    if pd.isna(left):\n",
        "        # print(f\"Mais proximo a direita: {right}\")\n",
        "        return grouped[right]\n",
        "    elif pd.isna(right):\n",
        "        # print(f\"Mais proximo a esquerda: {left}\")\n",
        "        return grouped[left]\n",
        "    else:\n",
        "        # print(f\"Inserido a media de {left} e {right} para {index}\")\n",
        "        return grouped[left] + grouped[right] / 2\n",
        "\n",
        "#funcao para gerar o dataset catch22 atraves do dataset_resampled\n",
        "def get_catch22(dataset_resampled):\n",
        "  df = dataset_resampled.drop([\"species\", \"age\"], axis=1)\n",
        "  dta_aux = []\n",
        "\n",
        "  for s in df.index:\n",
        "    series = df.iloc[s]\n",
        "    s = dataset_resampled.iloc[s]\n",
        "    species = s[\"species\"]\n",
        "    age = s[\"age\"]\n",
        "    values = pycatch22.catch22_all(series)[\"values\"] + [age, species]\n",
        "    dta_aux.append(values)\n",
        "\n",
        "  colunas = [1000 + i for i in range(22)] + [\"age\", \"species\"]\n",
        "  dataset_22 = pd.DataFrame(dta_aux, columns=colunas)\n",
        "\n",
        "  for col in colunas:\n",
        "    df[str(col)] = dataset_22[col]\n",
        "\n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFiCqDP8Hhtk"
      },
      "outputs": [],
      "source": [
        "def extrair_mzz_files():\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for name in files:\n",
        "            if name.endswith('.mzz'):\n",
        "                caminho_arquivo_mzz = os.path.join(root, name)\n",
        "                with zipfile.ZipFile(caminho_arquivo_mzz, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(root)\n",
        "\n",
        "                pasta_extraida = os.path.splitext(caminho_arquivo_mzz)[0]\n",
        "                os.remove(caminho_arquivo_mzz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Nwk0IIHhtk"
      },
      "outputs": [],
      "source": [
        "def find_age(splitted):\n",
        "    index = next((i for i, item in enumerate(splitted) if 'D' in item), -1)\n",
        "\n",
        "    if index != -1:\n",
        "        return int(splitted[index][:-1])\n",
        "    return int(splitted[1])\n",
        "\n",
        "def find_specie(splitted):\n",
        "    if splitted[0] == 'AA':\n",
        "        return 'AR'\n",
        "    elif splitted[0] == 'AR':\n",
        "        return 'AR'\n",
        "    elif splitted[0] == 'AG':\n",
        "        return 'KS'\n",
        "    elif splitted[0] == 'KS':\n",
        "        return 'KS'\n",
        "    return splitted[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbOj0mGUHhtl"
      },
      "outputs": [],
      "source": [
        "def extrair_mzz_files():\n",
        "  for root, dirs, files in os.walk(directory):\n",
        "      for name in files:\n",
        "          if name.endswith('.mzz'):\n",
        "              caminho_arquivo_mzz = os.path.join(root, name)\n",
        "\n",
        "              with zipfile.ZipFile(caminho_arquivo_mzz, 'r') as zip_ref:\n",
        "                  zip_ref.extractall(root)\n",
        "\n",
        "              pasta_extraida = os.path.splitext(caminho_arquivo_mzz)[0]\n",
        "              print(caminho_arquivo_mzz)\n",
        "\n",
        "              os.remove(pasta_extraida)\n",
        "\n",
        "#funcao para concatenar os datasets separados pelas threads\n",
        "def concat_datasets(folders):\n",
        "    features = []\n",
        "    resampleds = []\n",
        "    for folder in folders:\n",
        "        df_features = pd.read_csv(directory+'features_originais-'+folder+'.csv', sep=';')\n",
        "        df_resampled = pd.read_csv(directory+'dataset_resampled-'+folder+'.csv', sep=';')\n",
        "        features.append(df_features)\n",
        "        resampleds.append(df_resampled)\n",
        "\n",
        "    print('- Generating database features_originais')\n",
        "    concatenated_features = pd.concat([features[0], features[1], features[2]], ignore_index=True)\n",
        "    print('- Generating database dataset_resampled')\n",
        "    concatenated_resampled = pd.concat([resampleds[0], resampleds[1], resampleds[2]], ignore_index=True)\n",
        "    print('- Generating database dataset_catch22')\n",
        "    dataset_22 = get_catch22(concatenated_resampled)\n",
        "\n",
        "    print('- Saving dataset features_originais')\n",
        "    concatenated_features.to_csv(directory+'features_originais.csv', index=False, sep=';')\n",
        "    print('- Saving dataset dataset_resampled')\n",
        "    concatenated_resampled.to_csv(directory+'dataset_resampled.csv', index=False, sep=';')\n",
        "    print('- Saving dataset dataset_catch22')\n",
        "    dataset_22.to_csv(directory+'dataset_22.csv', index=False, sep=';')\n",
        "\n",
        "def calculate_wavenumber(a,b,c):\n",
        "  wvn_n = []\n",
        "  for i in range(0, int(c)):\n",
        "    wvn_i = a - (i * (a - b) / c )\n",
        "    wvn_n.append(wvn_i)\n",
        "  return wvn_n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIp-vISMHhtm"
      },
      "outputs": [],
      "source": [
        "def parte(foldername):\n",
        "    dataset = pd.DataFrame()\n",
        "    dataset_resampled = pd.DataFrame()\n",
        "    wavenumbers = [3856, 3400, 3275, 2923, 2859, 1901, 1746, 1636, 1539, 1457, 1307, 1154, 1076, 1027, 880, 526, 401]\n",
        "    for root_tmp, dirs_tmp, files_tmp in os.walk(directory):\n",
        "      if foldername in root_tmp:\n",
        "          print(f'\\n ------------------------------------------{foldername}: \\n{root_tmp}\\n------------------------------------------------')\n",
        "          for file_tmp in files_tmp:\n",
        "            if file_tmp.endswith('.tmp'):\n",
        "              hasAge = find_age(file_tmp.split('-'))\n",
        "              if file_tmp.split('-')[2] != 'UNK' and file_tmp.split('-')[0] != 'AC' and file_tmp.split('-')[0] != 'ACB' and file_tmp.split('-')[2] != '99D' and hasAge != -1:\n",
        "                caminho_arquivo_tmp = os.path.join(root_tmp, file_tmp)\n",
        "\n",
        "                df = pd.read_csv(caminho_arquivo_tmp, sep='\\s+', header=None)\n",
        "                df.columns = ['absorbance']\n",
        "\n",
        "                a = df['absorbance'].iloc[0]\n",
        "                b = df['absorbance'].iloc[1]\n",
        "                c = df['absorbance'].iloc[2]\n",
        "\n",
        "                df_absorbance = df.iloc[3:].copy()\n",
        "                df_absorbance['wavenumber'] = calculate_wavenumber(a,b,c)\n",
        "\n",
        "                #dataset que sera gerado com 1000 colunas mas com todos atributos referentes ao wavenumber\n",
        "                data_signal = pd.Series(znorm(signal.resample(df_absorbance['absorbance'], 1000))) #downsampling do sinal de 15000 ou 1700 para 1000 colunas\n",
        "                data_signal['species'] = find_specie(file_tmp.split('-')) # adiciona a espécie na penúltima coluna\n",
        "                data_signal['age'] = find_age(file_tmp.split('-')) # adiciona a idade (int) na última coluna\n",
        "                data_resampled = pd.DataFrame(data_signal).transpose() # adiciona serie temporal com normalizacao ao dataframe data_resampled\n",
        "                dataset_resampled = dataset_resampled.append(data_resampled, ignore_index=True) # anexa o conjunto referente a esse arquivo ao dataset final\n",
        "\n",
        "                df_absorbance['wavenumber'] = df_absorbance['wavenumber'].astype(int) # realiza conversão dos valores referentes ao wavenumber para inteiro\n",
        "                grouped = df_absorbance.groupby('wavenumber')['absorbance'].mean() # formata em grupos os wavenumbers iguais e realiza media para cada grupo\n",
        "                #checagem para verificar se os valores de absorbance referentes aos wavenumbers selecionados estao no dataset\n",
        "                for index in wavenumbers:\n",
        "                  #caso nao esteja atribui o valor mais proximo para absorbance referente aquele wavenumber\n",
        "                  if not grouped.index.isin([index]).any():\n",
        "                    new_absorbance = closest_index(grouped, index)\n",
        "                    grouped.loc[index] = new_absorbance\n",
        "\n",
        "                grouped = grouped.sort_index()\n",
        "                grouped = grouped[grouped.index.isin(wavenumbers)]\n",
        "                #separa os valores do wavenumber como header do dataset e absorbance como values normalizados.\n",
        "                dataset_key_value = pd.DataFrame({'header': grouped.index, 'value': znorm(grouped.values)})\n",
        "                dataset_key_value.set_index('header', inplace=True)\n",
        "                dataset_key_value = dataset_key_value.T\n",
        "                dataset_key_value['species'] = find_specie(file_tmp.split('-'))\n",
        "                dataset_key_value['age'] = find_age(file_tmp.split('-'))\n",
        "                dataset = dataset.append(dataset_key_value, ignore_index=True)\n",
        "\n",
        "    #transforma o dataset padrão com 17 atributos em arquivo csv\n",
        "    dataset['age'] = pd.cut(dataset['age'], bins=[1, 5, 11, 18], labels=['1-4', '5-10', '11-17'], right=False)\n",
        "    dataset.to_csv(directory+'features_originais-'+foldername+'.csv', index=False, sep=';')\n",
        "\n",
        "    #transforma o dataset que foi feito resampled com todos atributos em arquivo csv\n",
        "    dataset_resampled['age'] = pd.cut(dataset_resampled['age'], bins=[1, 5, 11, 18], labels=['1-4', '5-10', '11-17'], right=False)\n",
        "    dataset_resampled.to_csv(directory+'dataset_resampled-'+foldername+'.csv', index=False, sep=';')\n",
        "\n",
        "    #cria e gera um dataset utilizando pycatch22 para extrair 22 atributos\n",
        "    # dataset_22 = get_catch22(dataset_resampled)\n",
        "    # dataset_22.to_csv(directory+'dataset_22-'+foldername+'.csv', index=False, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiF4o_PPHhtm"
      },
      "outputs": [],
      "source": [
        "folders = [\"1. Laboratory Variation\", \"2. Genetic Variation\", \"3. Environmental Variation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELSfA-FaHhtn"
      },
      "outputs": [],
      "source": [
        "thread1 = threading.Thread(target=parte, args=(folders[0],))\n",
        "thread2 = threading.Thread(target=parte, args=(folders[1],))\n",
        "thread3 = threading.Thread(target=parte, args=(folders[2],))\n",
        "\n",
        "thread1.start()\n",
        "thread2.start()\n",
        "thread3.start()\n",
        "\n",
        "thread1.join()\n",
        "thread2.join()\n",
        "thread3.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aS65oGeHhtn"
      },
      "outputs": [],
      "source": [
        "#concatena os datasets para um unico dataset\n",
        "concat_datasets(folders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch-dtiumHhtn"
      },
      "outputs": [],
      "source": [
        "data_features = pd.read_csv(directory+'features_originais.csv', sep=';')\n",
        "\n",
        "print(data_features.groupby(['age']).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI4bXIQ9Hhtn"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn8CnfiVHhtn"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import  model_selection\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "parameters_KNN = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree'],\n",
        "    'leaf_size': [5, 10, 20, 30],\n",
        "    'p': [1, 2],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "  }\n",
        "\n",
        "\n",
        "parameters_LR = {\n",
        "    'penalty': [None,'l1', 'l2', 'elasticnet'],\n",
        "    'C': [0.1, 1.0, 5.0],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'max_iter': [100, 200, 300, 500],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "parameters_SVC = {\n",
        "    'C': [0.1, 0.5, 1, 3, 5],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "parameters_RF = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 7, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "parameters_XGB = {\n",
        "    'max_depth': [1, 3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.01],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'gamma': [0, 0.1],\n",
        "    'min_child_weight': [1, 3, 5]\n",
        "}\n",
        "\n",
        "def classificar(name_file, file, X, y) -> pd.DataFrame:\n",
        "\n",
        "    # separando uma parte para base de validação (30%)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=42, stratify=y)\n",
        "\n",
        "    clfKNN = KNeighborsClassifier()\n",
        "    clfLR = LogisticRegression()\n",
        "    clfSVC = SVC()\n",
        "    clfRF = RandomForestClassifier()\n",
        "    clfXGB = XGBClassifier()\n",
        "\n",
        "    folds = 10\n",
        "\n",
        "    # Define as configurações para o GridSearchCV\n",
        "    grid_searches = [\n",
        "        (clfKNN, parameters_KNN),\n",
        "        (clfLR, parameters_LR),\n",
        "        (clfSVC, parameters_SVC),\n",
        "        (clfRF, parameters_RF),\n",
        "        (clfXGB, parameters_XGB)\n",
        "    ]\n",
        "\n",
        "    def run_grid_search(clf, params):\n",
        "        gs = GridSearchCV(clf, params, scoring='accuracy', cv=folds, n_jobs=-1)\n",
        "        gs.fit(X_train, y_train)\n",
        "        return gs\n",
        "\n",
        "    # Executa os GridSearch em paralelo\n",
        "    results = joblib.Parallel(n_jobs=-1)(\n",
        "        joblib.delayed(run_grid_search)(clf, params) for clf, params in grid_searches\n",
        "    )\n",
        "\n",
        "    # Extrai os resultados de cada classificador\n",
        "    df_results = []\n",
        "    best_estimators = []\n",
        "    for i, (clf, _) in enumerate(grid_searches):\n",
        "        gs = results[i]\n",
        "        df_results.append(gs.cv_results_)\n",
        "        best_estimators.append(gs.best_estimator_)\n",
        "\n",
        "        print(f\"Best params for {clf.__class__.__name__}: \")\n",
        "        print(gs.best_params_)\n",
        "\n",
        "    # Utiliza os melhores estimadores para fazer as previsões\n",
        "    y_preds = []\n",
        "    accuracies = []\n",
        "    matriz_confusoes = []\n",
        "    for clf, y_pred_name in zip(best_estimators, ['KNN', 'LR', 'SVC', 'RF', 'XGB']):\n",
        "        y_pred = clf.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        matriz_confusao = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "        print(f\"\\n{y_pred_name}: \\n\")\n",
        "        print(\"Acuracia: {:.5f}\".format(accuracy))\n",
        "        print(\"Matriz de Confusão - {}: \".format(y_pred_name))\n",
        "        print(matriz_confusao)\n",
        "\n",
        "        y_preds.append(y_pred)\n",
        "        accuracies.append(accuracy)\n",
        "        matriz_confusoes.append(matriz_confusao)\n",
        "\n",
        "    resultados = {\n",
        "        'Classificador': ['KNN', 'Logistic Regression', 'SVC', 'Random Forest', 'XGBoost'],\n",
        "        'Melhores Parâmetros': [gs.best_params_ for gs in results],\n",
        "        'Melhor Acurácia': accuracies\n",
        "    }\n",
        "\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "\n",
        "    df_resultados.to_csv(root + f'resultados_classificadores_{file}.csv', sep=';', index=False)\n",
        "\n",
        "    return df_resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qg5XfrzHhto"
      },
      "outputs": [],
      "source": [
        "files = ['features_originais']\n",
        "dfs = []\n",
        "\n",
        "for file in files:\n",
        "    name_file = directory + f\"{file}.csv\"\n",
        "\n",
        "    data_age = pd.read_csv(name_file, sep=';')\n",
        "    data_age['age'] = data_age['age'].map({'1-4': 0, '5-10': 1, '11-17': 2})\n",
        "    print(f\"\\n\\n\\n\\nLendo dataset {name_file}........\")\n",
        "\n",
        "    X = data_age.drop([ 'age', 'species'], axis=1)\n",
        "    y = data_age['age']\n",
        "    df_gerado = classificar(name_file, file, X, y)\n",
        "\n",
        "    df_gerado[\"dataset\"] = file\n",
        "    dfs.append(df_gerado)\n",
        "\n",
        "merged_dataset = pd.concat(dfs, axis=1)\n",
        "merged_dataset.to_csv(directory + f'resultados_classificadores_idades.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5_ZZ-BWHhto"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}